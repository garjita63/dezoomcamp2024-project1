{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5b4cde",
   "metadata": {},
   "source": [
    "## Pyspark SQL -- BigQuery (read/write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d8a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/01 01:26:09 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/04/01 01:26:09 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/04/01 01:26:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/04/01 01:26:09 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----+------+-------------+\n",
      "|          timestamp|visitorid|event|itemid|transactionid|\n",
      "+-------------------+---------+-----+------+-------------+\n",
      "|2015-06-02 05:02:12|   257597| view|355908|            0|\n",
      "|2015-06-02 05:50:14|   992329| view|248676|            0|\n",
      "|2015-06-02 05:13:19|   111016| view|318965|            0|\n",
      "|2015-06-02 05:12:35|   483717| view|253185|            0|\n",
      "|2015-06-02 05:02:17|   951259| view|367447|            0|\n",
      "|2015-06-02 05:48:06|   972639| view| 22556|            0|\n",
      "|2015-06-02 05:12:03|   810725| view|443030|            0|\n",
      "|2015-06-02 05:34:51|   794181| view|439202|            0|\n",
      "|2015-06-02 04:54:59|   824915| view|428805|            0|\n",
      "|2015-06-02 05:00:04|   339335| view| 82389|            0|\n",
      "|2015-06-02 05:16:02|   176446| view| 10572|            0|\n",
      "|2015-06-02 05:08:21|   929206| view|410676|            0|\n",
      "|2015-06-02 05:50:29|    15795| view| 44872|            0|\n",
      "|2015-06-02 05:41:37|   598426| view|156489|            0|\n",
      "|2015-06-02 05:47:58|   223343| view|402625|            0|\n",
      "|2015-06-02 05:22:11|    57036| view|334662|            0|\n",
      "|2015-06-02 05:33:59|  1377281| view|251467|            0|\n",
      "|2015-06-02 05:50:44|  1370216| view|176721|            0|\n",
      "|2015-06-02 05:47:50|  1398644| view|135256|            0|\n",
      "|2015-06-02 05:50:52|   653756| view|132316|            0|\n",
      "+-------------------+---------+-----+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Transform events table into events_dwh\n",
    "# from_unixtime --> datetime (yyyy-MM-DD HH:mm:ss)\n",
    "# Run in Cloud Jupyter Notebook\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# credentials_location = '/root/.google/credentials/google-creds.json'\n",
    "\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('events') \\\n",
    "    .set(\"spark.jars\", \"/usr/lib/spark/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \n",
    "#    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "project_id = \"<project name>\"\n",
    "dataset_id = \"project1\"\n",
    "table_source = \"events\"\n",
    "\n",
    "df = spark.read.format('bigquery') \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"dataset\", dataset_id) \\\n",
    "    .load(table_source)\n",
    "    \n",
    "df.createOrReplaceTempView(\"temp_events\")\n",
    "\n",
    "events_transform = spark.sql(\"\"\"\n",
    "select from_unixtime((timestamp / 1000), \"yyyy-MM-dd HH:mm:ss\") as timestamp, \n",
    "    visitorid, event, itemid, transactionid\n",
    "from temp_events\n",
    "\"\"\")\n",
    "\n",
    "events_transform.show()\n",
    "\n",
    "project_id = \"<project name>\"\n",
    "dataset_id = \"project1\"\n",
    "table_target = \"events_dwh\"\n",
    "parttition_column = \"DATE_FORMAT(timestamp, 'yyyy-MM')\"\n",
    "cluster_column = \"event\"\n",
    "\n",
    "events_transform.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"table\", f\"{project_id}.{dataset_id}.{table_target}\") \\\n",
    "    .option(\"PARTITION BY\",  parttition_column) \\\n",
    "    .option(\"CLUSTER BY\", cluster_column) \\\n",
    "    .mode('Overwrite') \\\n",
    "    .save()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 01:29:59 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/04/01 01:29:59 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/04/01 01:29:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/04/01 01:29:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----------+--------------------+\n",
      "|          timestamp|itemid|  property|               value|\n",
      "+-------------------+------+----------+--------------------+\n",
      "|2015-06-28 03:00:00|460429|categoryid|                1338|\n",
      "|2015-09-06 03:00:00|206783|       888|1116713 960601 n2...|\n",
      "|2015-08-09 03:00:00|395014|       400|n552.000 639502 n...|\n",
      "|2015-05-10 03:00:00| 59481|       790|          n15360.000|\n",
      "|2015-05-17 03:00:00|156781|       917|              828513|\n",
      "|2015-07-05 03:00:00|285026| available|                   0|\n",
      "|2015-06-14 03:00:00| 89534|       213|             1121373|\n",
      "|2015-05-17 03:00:00|264312|         6|              319724|\n",
      "|2015-06-07 03:00:00|229370|       202|             1330310|\n",
      "|2015-06-14 03:00:00| 98113|       451|     1141052 n48.000|\n",
      "|2015-08-09 03:00:00|450113|       888|1038400 45956 n50...|\n",
      "|2015-06-28 03:00:00|244127|       400|n552.000 639502 n...|\n",
      "|2015-08-16 03:00:00|264319|       227|      1283144 353870|\n",
      "|2015-08-16 03:00:00|348323|       839|     1026952 1162729|\n",
      "|2015-06-14 03:00:00|169055|       790|          n21000.000|\n",
      "|2015-07-19 03:00:00|186518| available|                   0|\n",
      "|2015-06-28 03:00:00|178601|       790|           n5400.000|\n",
      "|2015-07-12 03:00:00|319291|       888|             1292080|\n",
      "|2015-07-19 03:00:00| 49337|         0|n36.000 1186610 1...|\n",
      "|2015-06-28 03:00:00|363598|      1022|       857891 593337|\n",
      "+-------------------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "# Transform item_properties table into item_properties_dwh\n",
    "# from_unixtime --> datetime (yyyy-MM-DD HH:mm:ss)\n",
    "# Run in Cloud Jupyter Notebook\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# credentials_location = '/root/.google/credentials/google-creds.json'\n",
    "\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('item_properties') \\\n",
    "    .set(\"spark.jars\", \"/usr/lib/spark/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \n",
    "#    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "project_id = \"<project name>\"\n",
    "dataset_id = \"project1\"\n",
    "table_source = \"item_properties\"\n",
    "\n",
    "df = spark.read.format('bigquery') \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"dataset\", dataset_id) \\\n",
    "    .load(table_source)\n",
    "    \n",
    "df.createOrReplaceTempView(\"temp_item_properties\")\n",
    "\n",
    "item_properties_transform = spark.sql(\"\"\"\n",
    "select from_unixtime((timestamp / 1000), \"yyyy-MM-dd HH:mm:ss\") as timestamp, \n",
    "    itemid, property, value\n",
    "from temp_item_properties\n",
    "\"\"\")\n",
    "\n",
    "item_properties_transform.show()\n",
    "\n",
    "project_id = \"<project name>\"\n",
    "dataset_id = \"project1\"\n",
    "table_target = \"item_properties_dwh\"\n",
    "parttition_column = \"DATE_FORMAT(timestamp, 'yyyy-MM')\"\n",
    "cluster_column = \"property\"\n",
    "\n",
    "item_properties_transform.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"table\", f\"{project_id}.{dataset_id}.{table_target}\") \\\n",
    "    .option(\"PARTITION BY\",  parttition_column) \\\n",
    "    .option(\"CLUSTER BY\", cluster_column) \\\n",
    "    .mode('Overwrite') \\\n",
    "    .save()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93e6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
