{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5b4cde",
   "metadata": {},
   "source": [
    "## Pyspark SQL -- BigQuery (read/write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d8a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/01 01:12:23 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/04/01 01:12:23 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/04/01 01:12:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/04/01 01:12:24 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Transform events table into events_dwh\n",
    "# from_unixtime --> datetime (yyyy-MM-DD HH:mm:ss)\n",
    "# Run in Cloud Jupyter Notebook\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# credentials_location = '/root/.google/credentials/google-creds.json'\n",
    "\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('events') \\\n",
    "    .set(\"spark.jars\", \"/usr/lib/spark/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \n",
    "#    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "project_id = \"semar-de-project1\"\n",
    "dataset_id = \"project1\"\n",
    "table_source = \"events\"\n",
    "\n",
    "df = spark.read.format('bigquery') \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"dataset\", dataset_id) \\\n",
    "    .load(table_source)\n",
    "    \n",
    "df.createOrReplaceTempView(\"temp_events\")\n",
    "\n",
    "events_transform = spark.sql(\"\"\"\n",
    "select from_unixtime((timestamp / 1000), \"yyyy-MM-dd HH:mm:ss\") as timestamp, \n",
    "    visitorid, event, itemid, transactionid\n",
    "from temp_events\n",
    "\"\"\")\n",
    "\n",
    "# events_transform.show()\n",
    "\n",
    "project_id = \"semar-de-project1\"\n",
    "dataset_id = \"project1\"\n",
    "table_target = \"events_dwh\"\n",
    "parttition_column = \"DATE_FORMAT(timestamp, 'yyyy-MM')\"\n",
    "cluster_column = \"event\"\n",
    "\n",
    "events_transform.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"table\", f\"{project_id}.{dataset_id}.{table_target}\") \\\n",
    "    .option(\"PARTITION BY\",  parttition_column) \\\n",
    "    .option(\"CLUSTER BY\", cluster_column) \\\n",
    "    .mode('Overwrite') \\\n",
    "    .save()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f2ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 01:13:21 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/04/01 01:13:21 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/04/01 01:13:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/04/01 01:13:21 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/04/01 01:13:23 ERROR ApplicationMaster: Uncaught exception: \n",
      "org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1711926671331_0006_000001\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:362)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:260)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:101)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[?:?]\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateYarnException(RPCUtil.java:75) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:116) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:110) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat com.sun.proxy.$Proxy38.registerApplicationMaster(Unknown Source) ~[?:?]\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:247) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:234) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:214) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:72) ~[spark-yarn_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:432) [spark-yarn_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster.runUnmanaged(ApplicationMaster.scala:307) [spark-yarn_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anon$3.run(Client.scala:1218) [spark-yarn_2.12-3.5.0.jar:3.5.0]\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException: Application doesn't exist in cache appattempt_1711926671331_0006_000001\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:362)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:260)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:101)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1567) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1513) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1410) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat com.sun.proxy.$Proxy37.registerApplicationMaster(Unknown Source) ~[?:?]\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:108) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "# Transform item_properties table into item_properties_dwh\n",
    "# from_unixtime --> datetime (yyyy-MM-DD HH:mm:ss)\n",
    "# Run in Cloud Jupyter Notebook\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# credentials_location = '/root/.google/credentials/google-creds.json'\n",
    "\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('item_properties') \\\n",
    "    .set(\"spark.jars\", \"/usr/lib/spark/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \n",
    "#    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "project_id = \"semar-de-project1\"\n",
    "dataset_id = \"project1\"\n",
    "table_source = \"item_properties\"\n",
    "\n",
    "df = spark.read.format('bigquery') \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"dataset\", dataset_id) \\\n",
    "    .load(table_source)\n",
    "    \n",
    "df.createOrReplaceTempView(\"temp_item_properties\")\n",
    "\n",
    "item_properties_transform = spark.sql(\"\"\"\n",
    "select from_unixtime((timestamp / 1000), \"yyyy-MM-dd HH:mm:ss\") as timestamp, \n",
    "    itemid, property, value\n",
    "from temp_item_properties\n",
    "\"\"\")\n",
    "\n",
    "# item_properties_transform.show()\n",
    "\n",
    "project_id = \"semar-de-project1\"\n",
    "dataset_id = \"project1\"\n",
    "table_target = \"item_properties_dwh\"\n",
    "parttition_column = \"DATE_FORMAT(timestamp, 'yyyy-MM')\"\n",
    "cluster_column = \"property\"\n",
    "\n",
    "item_properties_transform.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\",\"dataproc-temp-asia-southeast2-212352110204-1oi7hped\") \\\n",
    "    .option(\"table\", f\"{project_id}.{dataset_id}.{table_target}\") \\\n",
    "    .option(\"PARTITION BY\",  parttition_column) \\\n",
    "    .option(\"CLUSTER BY\", cluster_column) \\\n",
    "    .mode('Overwrite') \\\n",
    "    .save()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c0774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
